{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cbc12ac-9487-4f3d-ba61-98e69553e22f",
   "metadata": {},
   "source": [
    "Adapted the following example to work with my input files and to use GraphSAGE:\n",
    "\n",
    "https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb?usp=sharing\n",
    "\n",
    "Training a GNN for graph classification usually follows a simple recipe:\n",
    "1. Embed each node by performing multiple rounds of message passing\n",
    "2. Aggregate node embeddings into a unified graph embedding (readout layer)\n",
    "3. Train a final classifier on the graph embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb6557-548d-4a3e-86b7-d7591e427f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "import time\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "# from torch_geometric.nn import GCNConv, global_add_pool\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "# import dgl\n",
    "import torch.nn as nn\n",
    "# import itertools\n",
    "# import scipy.sparse as sp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns  # For a nicer looking matrix\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe2420e-9645-4cad-a55b-2516e2505f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseX(xPos):\n",
    "    xPos = (xPos +350)/100\n",
    "    return xPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4fc482-1ca8-4779-8991-0a39e735a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseY(yPos):\n",
    "    yPos = (yPos +650)/100\n",
    "    return yPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f82a77-9b78-46d4-91a7-29046bdaa331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseZ(zPos):\n",
    "    zPos = zPos/100\n",
    "    return zPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e07d89-e1af-4ac7-ad60-74bed84dd81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseAdc(adcCounts):\n",
    "    adcCounts=adcCounts/10\n",
    "    return adcCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e55e0a-23e1-44b9-84c8-dc34cec25c44",
   "metadata": {},
   "source": [
    "### This is the function that loops over the input csv file lines.\n",
    "It calls *process_event* which unpacks each line in the file (each line = 1 cluster = 1 graph) to return an \"eventArray\" with features, edge labels, edge source and edge destination node IDs. One line = 1 cluster. It passes it each row of the csv file via \"data\" and also passes empty lists for x, y, z, and adcCounts, which will be filled by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa4c99-f303-4f02-9602-54b7d55bcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_file):\n",
    "    n_tracks_in_sample=0\n",
    "    n_showers_in_sample=0\n",
    "    with open(input_file, 'r') as f:\n",
    "        num_events = len(f.readlines())\n",
    "        print(\"num_events = \",num_events)\n",
    "    \n",
    "    dataset=[]\n",
    "    xPos=[]\n",
    "    yPos=[]\n",
    "    zPos=[]\n",
    "    adcCounts=[]\n",
    "    truePdg=[]\n",
    "    trueMCIndex=[]\n",
    "    with open(input_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for i, row in enumerate(tqdm(reader, desc=\"Test\", miniters=100, total=num_events)):\n",
    "                data = row[1:]\n",
    "                print(\"i = \",i,\" -----------------------------------------------------------------\")\n",
    "                \n",
    "                eventArray=process_event(data, f\"{i}\", xPos, yPos, zPos, adcCounts, truePdg, trueMCIndex)\n",
    "\n",
    "                if eventArray==[]:\n",
    "                    continue\n",
    "\n",
    "                features=eventArray[0]\n",
    "                edgeStart=eventArray[1]\n",
    "                edgeEnd=eventArray[2]\n",
    "                label=eventArray[3]\n",
    "\n",
    "                if len(edgeStart) == 0:\n",
    "                    continue\n",
    "                edge_index = torch.tensor([edgeStart, edgeEnd], dtype=torch.long)\n",
    "                node_features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "                data = Data(x=node_features, edge_index=edge_index)\n",
    "                \n",
    "                data.y = torch.tensor([label])\n",
    "\n",
    "                print(\"n_tracks_in_sample = \", n_tracks_in_sample, \" n_showers_in_sample = \",n_showers_in_sample, \" n_tracks_in_sample_max  = \",n_tracks_in_sample_max,\" n_showers_in_sample_max = \",n_showers_in_sample_max)\n",
    "                if (n_tracks_in_sample >= n_tracks_in_sample_max) and (n_showers_in_sample >= n_showers_in_sample_max):\n",
    "                    return dataset\n",
    "                elif (data.y == 0) and (n_showers_in_sample >= n_showers_in_sample_max):\n",
    "                    continue\n",
    "                elif (data.y == 1) and (n_tracks_in_sample >= n_tracks_in_sample_max):\n",
    "                    continue\n",
    "                #\n",
    "                if data.y == 0:\n",
    "                    n_showers_in_sample=n_showers_in_sample+1\n",
    "                elif data.y == 1:\n",
    "                    n_tracks_in_sample=n_tracks_in_sample+1\n",
    "                dataset.append(data)\n",
    "                print(\"len(dataset)= \",len(dataset))\n",
    "            \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcd7f2a-6b28-4ffd-b753-8fb8967f581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 = shower, 1 = track, -999 = other\n",
    "def find_true_particle_label(hpdg):\n",
    "    \n",
    "    label = -999\n",
    "    most_frequent_pdg = -999\n",
    "    unique_values, counts = np.unique(hpdg, return_counts=True)\n",
    "    \n",
    "    sorted_indices = np.argsort(counts)[::-1]\n",
    "    sorted_unique_values = unique_values[sorted_indices]\n",
    "    sorted_counts = counts[sorted_indices]\n",
    "    \n",
    "    # Most frequent PDG code and its count\n",
    "    most_frequent_pdg = int(sorted_unique_values[0])\n",
    "    most_frequent_count = sorted_counts[0]\n",
    "\n",
    "    if most_frequent_pdg == 22 or most_frequent_pdg == 11:\n",
    "        label = 0\n",
    "    elif most_frequent_pdg == 13 or most_frequent_pdg == 2212 or most_frequent_pdg == 321 or most_frequent_pdg == 211:\n",
    "        label = 1\n",
    "\n",
    "    return int(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9dcd8-db8d-43b6-a7e3-df2a0fc38aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_input(x,y,z,adc,mc,pdg):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # x-y plot\n",
    "    axes[0].scatter(x, y, c=pdg, cmap='tab20')\n",
    "    axes[0].set_title(\"X-Y Plot\")\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "\n",
    "    # x-z plot\n",
    "    axes[1].scatter(x, z, c=pdg, cmap='tab20')\n",
    "    axes[1].set_title(\"X-Z Plot\")\n",
    "    axes[1].set_xlabel(\"X\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "\n",
    "    # y-z plot\n",
    "    axes[2].scatter(y, z, c=pdg, cmap='tab20')\n",
    "    axes[2].set_title(\"Y-Z Plot\")\n",
    "    axes[2].set_xlabel(\"Y\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41ced3-5c2a-4bda-b00f-28535c199ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_evolution(epochs, training_loss, test_loss, accuracy, ylim1, ylim2):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(epochs, training_loss, label='Training Loss', color='b')\n",
    "    ax1.plot(epochs, test_loss, label='Validation Loss', color='g')\n",
    "\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Training and Validation Loss Over Epochs')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, accuracy, label='Accuracy', color='r')\n",
    "    ax2.set_ylabel('Accuracy Scale', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    if ylim1 > 0:\n",
    "        ax1.set_ylim(0.0001, ylim1)\n",
    "    if ylim2 > 0:\n",
    "        ax2.set_ylim(0.0001, ylim2)\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86927438-0ad9-44e1-9ea7-847ed2ed269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(data, event, xPos, yPos, zPos, adcCounts, truePdg, trueMCIndex):\n",
    "    eventArray=[]\n",
    "    nh_coords = 6\n",
    "    n_hits = int(data[0])\n",
    "    h_start, h_finish = 1, nh_coords * n_hits+1\n",
    "    length = len(data[h_start:-1]) \n",
    "    \n",
    "    if length != (n_hits * nh_coords):\n",
    "        print('Missing information in input file')\n",
    "        print(n_hits, length)\n",
    "        return\n",
    "    \n",
    "    hmc = np.array(data[h_start:h_finish:nh_coords], dtype=int)   #true mc index\n",
    "    hpdg = np.array(data[h_start+1:h_finish:nh_coords], dtype=float)   #true mc index\n",
    "    hx = np.array(data[h_start+2:h_finish:nh_coords], dtype=float)  #x coord\n",
    "    hy = np.array(data[h_start+3:h_finish:nh_coords], dtype=float)  #y coord  \n",
    "    hz = np.array(data[h_start+4:h_finish:nh_coords], dtype=float)  #z coord\n",
    "    hadc = np.array(data[h_start+5:h_finish:nh_coords], dtype=float)#adc\n",
    "    \n",
    "    if normalise_positions==1:\n",
    "        hx=normaliseX(hx)\n",
    "        hy=normaliseY(hy)\n",
    "        hz=normaliseZ(hz)\n",
    "        \n",
    "    if normalise_adcs==1:  \n",
    "        hadc=normaliseAdc(hadc)\n",
    "\n",
    "    print(\"n_hits = \",n_hits)\n",
    "    \n",
    "    if (n_hits > n_hits_in_graph_max) or (n_hits < n_hits_in_graph_min):\n",
    "        print(\"n hits outside allowed interval\")\n",
    "        return eventArray\n",
    "    \n",
    "    features=np.empty((n_hits,6))\n",
    "\n",
    "    dimension = n_hits*n_hits\n",
    "    edgeStart = np.zeros((dimension))\n",
    "    edgeEnd = np.zeros((dimension))\n",
    "    index=0\n",
    "\n",
    "    #Check that there is less than minPurity contamination\n",
    "    unique_values, counts = np.unique(hmc, return_counts=True)\n",
    "    if len(unique_values) >= 2:\n",
    "        sorted_counts = np.sort(counts)  # Sort the counts\n",
    "        #second_most_frequent_count = sorted_counts[-2]  # Get the second highest count\n",
    "        total_count = np.sum(counts)\n",
    "        if sorted_counts[-1] / total_count < minPurity:\n",
    "            print(\"The most contributing MC true particle in this cluster contributes less than minPurity hits\")\n",
    "            return eventArray\n",
    "\n",
    "    visualise_input(hx,hy,hz,hadc,hmc,hpdg)\n",
    "    \n",
    "    true_particle_label = find_true_particle_label(hpdg)\n",
    "    print(\"true_particle_label = \",true_particle_label)\n",
    "    if true_particle_label != 0 and true_particle_label !=1:\n",
    "        print(\"true_particle_label of wrong type = \",true_particle_label)\n",
    "        return eventArray\n",
    "\n",
    "\n",
    "\n",
    "    for hit1 in range(0,n_hits):\n",
    "        features[hit1,0]=hx[hit1]\n",
    "        features[hit1,1]=hy[hit1]\n",
    "        features[hit1,2]=hz[hit1]\n",
    "        features[hit1,3]=hadc[hit1]\n",
    "        features[hit1,4]=hmc[hit1]\n",
    "        features[hit1,5]=hpdg[hit1]\n",
    "        for hit2 in range(hit1+1,n_hits): \n",
    "            dx = hx[hit1] - hx[hit2]\n",
    "            dy = hy[hit1] - hy[hit2]\n",
    "            dz = hz[hit1] - hz[hit2]\n",
    "            dist = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "            if (dist>max_hit_distance_radius):\n",
    "                continue            \n",
    "            edgeStart[index]=int(hit1)\n",
    "            edgeEnd[index]=int(hit2)\n",
    "            index=index+1\n",
    "\n",
    "    #Resize the arrays to contain entries for the hits within that range.\n",
    "    edgeStart = edgeStart[:index]\n",
    "    edgeEnd = edgeEnd[:index]\n",
    "    features = features[:index,:]        \n",
    "    eventArray=[features,edgeStart,edgeEnd,true_particle_label]\n",
    "    return eventArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6185c-4569-4b28-a3a2-31288dded64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_classes):\n",
    "        super(FinalClassifier, self).__init__()\n",
    "        self.lin = Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4c426-0cf3-4aa5-b9f5-f1be3d444f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readout layer\n",
    "class ReadoutLayer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReadoutLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5175f-6eb7-464a-83ec-f45e1a1986cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn(dataset):\n",
    "\n",
    "    test_graphs_size=int(len(dataset)*test_train_ratio)\n",
    "    train_graphs_size=int(len(dataset)-test_graphs_size)\n",
    "    print(\"train_graphs_size = \",train_graphs_size,\" test_graphs_size = \",test_graphs_size,\" len(dataset)= \",len(dataset))\n",
    "\n",
    "    #Split dataset into train and test samples of graphs randomly:\n",
    "    indices = np.random.choice(len(dataset), test_graphs_size, replace=False)\n",
    "    test_dataset = [dataset[i] for i in indices]\n",
    "    mask = np.ones(len(dataset), dtype=bool)\n",
    "    mask[indices] = False\n",
    "    train_dataset = [dataset[i] for i in range(len(dataset)) if mask[i]]\n",
    "\n",
    "\n",
    "    print(f'Number of training graphs: {len(train_dataset)}')\n",
    "    print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=180, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=180, shuffle=False)\n",
    "\n",
    "    for step, data in enumerate(train_loader):\n",
    "        print(f'Step {step + 1}:')\n",
    "        print('=======')\n",
    "        print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "        print(data)\n",
    "        print()\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    dataset_num_classes=2 #Tracks and showers\n",
    "    \n",
    "    # This is using the pyg own module ##################\n",
    "    GraphSAGE_model = GraphSAGE(\n",
    "        dataset[0].num_node_features,\n",
    "        hidden_channels=model_hidden_channels,\n",
    "        num_layers=model_num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    ReadoutLayer_model = ReadoutLayer(\n",
    "    ).to(device)\n",
    "    \n",
    "    FinalClassifier_model = FinalClassifier(\n",
    "        hidden_channels=model_hidden_channels,\n",
    "        num_classes=dataset_num_classes\n",
    "        \n",
    "    ).to(device)\n",
    "    \n",
    "    # # ----------- 3. set up loss and optimizer -------------- #\n",
    "    model_parameters = list(GraphSAGE_model.parameters()) + list(ReadoutLayer_model.parameters()) + list(FinalClassifier_model.parameters())\n",
    "    optimizer = torch.optim.Adam(model_parameters, lr=learning_rate)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    training_loss=0\n",
    "    test_loss=0\n",
    "    training_loss_array=[]\n",
    "    test_loss_array=[]\n",
    "    accuracy_array=[]\n",
    "    epochs_array=[]\n",
    "    test_true_labels=[]\n",
    "    test_predicted_labels=[]\n",
    "    for e in epochs:\n",
    "        test_correct=0\n",
    "        train_correct=0\n",
    "        train_acc=0\n",
    "        test_acc=0\n",
    "        print(\"epoch = \", e)\n",
    "\n",
    "        #Train\n",
    "        GraphSAGE_model.train()\n",
    "        ReadoutLayer_model.train()\n",
    "        FinalClassifier_model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        for step, data in enumerate(train_loader):\n",
    "            h = GraphSAGE_model(data.x, data.edge_index, data.batch)\n",
    "            h1 = ReadoutLayer_model(h, data.batch)\n",
    "            h2 = FinalClassifier_model(h1)\n",
    "            training_loss = criterion(h2, data.y)\n",
    "            training_loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "            pred = h2.argmax(dim=1)  # Use the class with highest probability.\n",
    "            train_correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        train_acc=train_correct / len(train_loader.dataset)\n",
    "            \n",
    "\n",
    "        #Test\n",
    "        with torch.no_grad():\n",
    "            GraphSAGE_model.eval()\n",
    "            ReadoutLayer_model.eval()\n",
    "            FinalClassifier_model.eval()\n",
    "        for step, data in enumerate(test_loader):\n",
    "        #for data in test_loader:  \n",
    "            h = GraphSAGE_model(data.x, data.edge_index, data.batch)\n",
    "            h1 = ReadoutLayer_model(h, data.batch)\n",
    "            h2 = FinalClassifier_model(h1)\n",
    "            test_loss = criterion(h2, data.y)\n",
    "            pred = h2.argmax(dim=1)  # Use the class with highest probability.\n",
    "            test_predicted_labels=pred\n",
    "            test_true_labels=data.y\n",
    "            test_correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "\n",
    "        test_acc=test_correct / len(test_loader.dataset)\n",
    "        training_loss_array.append(training_loss.detach().numpy())\n",
    "        test_loss_array.append(test_loss.detach().numpy())\n",
    "        accuracy_array.append(test_acc)\n",
    "        epochs_array.append(e)\n",
    "        \n",
    "        print(f'Epoch: {e:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    plot_loss_evolution(epochs_array, training_loss_array, test_loss_array, accuracy_array, -999,-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b846a0-c278-4a44-a789-3b290eb134d5",
   "metadata": {},
   "source": [
    "## Run the GNN! (Here are the commands to prepare samples and run the training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5c77c-5e82-4933-a5c0-6242d1045ea0",
   "metadata": {},
   "source": [
    "### Parameters (modify based on your needs!):\n",
    "**prepare_samples** : if the sample is already loaded in memory in the notebook, I don't want to accidentally re-prepare it (as it's time consuming), so I check this condition later in the notebook when I call prepare_samples_for_training\n",
    "\n",
    "**n_hits_in_graph_min, n_hits_in_graph_max** : minimum and maximum number of hits for cluster to be in input sample for training\n",
    "\n",
    "**minPurity** : fraction of hits in the cluster that are contributed by the most contributing MC particle. Only select tracks and showers with a purity larger than minPurity. \n",
    "\n",
    "**test_train_ratio**, **epochs**, **learning rate** should self-describe.\n",
    "\n",
    "***model_hidden_channels*** : the dimensionality of the hidden feature space in the GraphSAGE model\n",
    "\n",
    "***model_num_layers*** : number of message-passing layers in the GraphSAGE model\n",
    "\n",
    "***normalise_adcs***,***normalise_positions*** : bringing inputs down to a \"small number\" range. not normalised between 0 and 1, but numbers are broadly in the 1-10 range.\n",
    "\n",
    "***plot_input_distributions*** : if true, plot input node feature distributions (i.e. x,y,z,adc) plus some other \n",
    "\n",
    "***max_hit_distance_radius*** : in this edge-prediction problem, only create an edge (regardless of if positive or negative) for hits that are less than this amount apart. Units change based on normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42be5e-c4ca-44af-917b-521b8a945b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "prepare_samples_again=True\n",
    "\n",
    "n_hits_in_graph_min=200\n",
    "n_hits_in_graph_max=12000\n",
    "minPurity=0.95\n",
    "\n",
    "n_tracks_in_sample_max=200\n",
    "n_showers_in_sample_max=200\n",
    "\n",
    "test_train_ratio=0.1\n",
    "epochs = range(1, 100)  \n",
    "learning_rate=0.01\n",
    "model_hidden_channels=16\n",
    "model_num_layers=3\n",
    "\n",
    "#input_coord_normalisation=1\n",
    "normalise_adcs=1\n",
    "normalise_positions=1\n",
    "\n",
    "if normalise_positions:\n",
    "    max_hit_distance_radius=0.1 #this  is now in m so 10 cm \n",
    "else:\n",
    "    max_hit_distance_radius=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdce3f40-82ff-4470-aaa4-71a9a53c781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_samples_again:\n",
    "    #file_path = f\"training_out_test_feb_nue_43.csv\"\n",
    "    #file_path = f\"training_out_test_feb_nue_showers.csv\"\n",
    "    #file_path = f\"training_out_tsId.csv\"\n",
    "    #file_path = f\"InputForTrackShowerID_GNN_nu_and_nue.csv\" #huge file (100 files of nu and 100 files of nue)\n",
    "    file_path = f\"InputForTrackShowerID_GNN_nu_and_nue_20files.csv\" #large file (10 files of nu and 10 files of nue)\n",
    "    output_directory = f\"outputDir\"\n",
    "    dataset=process_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6bfbf6-298f-4188-8c88-4397dc1ead84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the GNN\n",
    "print(\"len(dataset) = \",len(dataset))\n",
    "train_gnn(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
