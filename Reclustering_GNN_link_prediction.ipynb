{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN for clustering\n",
    "\n",
    "This notebook trains a GraphSAGE GNN for a link-prediction problem.\n",
    "Inputs are Pandora 3D clusters that contain more than one true particle contributing a sizeable amount of the total energy.\n",
    "\n",
    "Nodes correspond to 3D hits, and node features are: x, y, z coordinates and ADC counts.\n",
    "\n",
    "A positive link should be predicted between any two hits that were contributed most ADC counts by the same true particle.\n",
    "\n",
    "GraphSAGE paper: https://arxiv.org/abs/1706.02216 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import os.path as osp\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions (shouldn't need to modify these for routine operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliseX(xPos):\n",
    "    xPos = (xPos +350)/100\n",
    "    return xPos\n",
    "\n",
    "def normaliseY(yPos):\n",
    "    yPos = (yPos +650)/100\n",
    "    return yPos\n",
    "\n",
    "def normaliseZ(zPos):\n",
    "    zPos = zPos/100\n",
    "    return zPos\n",
    "\n",
    "def normaliseAdc(adcCounts):\n",
    "    adcCounts=adcCounts/10\n",
    "    return adcCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the function that loops over the input csv file lines.\n",
    "It calls *process_event* which unpacks each line in the file (each line = 1 cluster = 1 graph) to return an \"eventArray\" with features, edge labels, edge source and edge destination node IDs. One line = 1 cluster. It passes it each row of the csv file via \"data\" and also passes empty lists for x, y, z, and adcCounts, which will be filled by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_file):\n",
    "    with open(input_file, 'r') as f:\n",
    "        num_events = len(f.readlines())\n",
    "    \n",
    "    dataset=[]\n",
    "    xPos=[]\n",
    "    yPos=[]\n",
    "    zPos=[]\n",
    "    adcCounts=[]\n",
    "\n",
    "    with open(input_file, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for i, row in enumerate(tqdm(reader, desc=\"Test\", miniters=100, total=num_events)):\n",
    "                data = row[1:]\n",
    "                #print(\"row nr. = \",i,\" -----------------------------------------------------------------\")\n",
    "                \n",
    "                eventArray=process_event(data, f\"{i}\", xPos, yPos, zPos, adcCounts)\n",
    "                \n",
    "                if(len(eventArray)==0):\n",
    "                    continue\n",
    "                features=eventArray[0]\n",
    "                #print(\"debug features = \",features)\n",
    "                edgeStart=eventArray[1]\n",
    "                edgeEnd=eventArray[2]\n",
    "                edgeLabel=eventArray[3]\n",
    "                if len(edgeStart) == 0:\n",
    "                    continue\n",
    "                edge_index = torch.tensor([edgeStart, edgeEnd], dtype=torch.long)\n",
    "                node_features = torch.tensor(features, dtype=torch.float)\n",
    "                edge_labels = torch.tensor(edgeLabel, dtype=torch.long)\n",
    "                \n",
    "                data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_labels)\n",
    "\n",
    "                #Define and then append subgraph with only positive edges\n",
    "                subg=filter_by_edge_label(data,1)\n",
    "\n",
    "                # Print info for debug please...######################\n",
    "                #print_graph_info(subg)\n",
    "                #####################################################\n",
    "\n",
    "                dataset.append(subg)\n",
    "\n",
    "            if plot_input_distributions is True:\n",
    "    \n",
    "                plt.hist(xPos, bins=50, alpha=0.5, color='blue')\n",
    "                plt.title('xPos')\n",
    "                plt.xlabel('xPos')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "    \n",
    "                plt.hist(yPos, bins=50, alpha=0.5, color='blue')\n",
    "                plt.title('yPos')\n",
    "                plt.xlabel('yPos')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "    \n",
    "                plt.hist(zPos, bins=50, alpha=0.5, color='blue')\n",
    "                plt.title('zPos')\n",
    "                plt.xlabel('zPos')\n",
    "                plt.grid(True)\n",
    "                plt.show()\n",
    "    \n",
    "                plt.hist(adcCounts, bins=50, alpha=0.5, color='blue')\n",
    "                plt.title('adcCounts')\n",
    "                plt.xlabel('adcCounts')\n",
    "                plt.grid(True)\n",
    "                plt.show()    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_event(data, event, xPos, yPos, zPos, adcCounts):\n",
    "    eventArray=[]\n",
    "    nh_coords = 5\n",
    "    n_hits = int(data[0])\n",
    "    h_start, h_finish = 1, nh_coords * n_hits+1\n",
    "    length = len(data[h_start:-1]) \n",
    "    \n",
    "    if length != (n_hits * nh_coords):\n",
    "        print('Missing information in input file')\n",
    "        print(n_hits, length)\n",
    "        return\n",
    "    \n",
    "    hmc = np.array(data[h_start:h_finish:nh_coords], dtype=int)   #true mc index\n",
    "    hx = np.array(data[h_start+1:h_finish:nh_coords], dtype=float)  #x coord\n",
    "    hy = np.array(data[h_start+2:h_finish:nh_coords], dtype=float)  #y coord  \n",
    "    hz = np.array(data[h_start+3:h_finish:nh_coords], dtype=float)  #z coord\n",
    "    hadc = np.array(data[h_start+4:h_finish:nh_coords], dtype=float)#adc\n",
    "\n",
    "    if normalise_positions==True:\n",
    "        hx=normaliseX(hx)\n",
    "        hy=normaliseY(hy)\n",
    "        hz=normaliseZ(hz)\n",
    "        \n",
    "    if normalise_adcs==True:  \n",
    "        hadc=normaliseAdc(hadc)\n",
    "    \n",
    "    if (n_hits > n_hits_in_graph_max) or (n_hits < n_hits_in_graph_min):\n",
    "       return eventArray\n",
    "    \n",
    "    features=np.empty((n_hits,5))\n",
    "\n",
    "    dimension = n_hits*n_hits\n",
    "    edgeLabel = np.zeros((dimension))\n",
    "    edgeStart = np.zeros((dimension))\n",
    "    edgeEnd = np.zeros((dimension))\n",
    "    index=0\n",
    "\n",
    "    #Check that not all hits are associated to the same particle! And the second most contributing particle contributes at least minFrac of hits:\n",
    "    unique_values, counts = np.unique(hmc, return_counts=True)\n",
    "    if len(unique_values) >= 2:\n",
    "        sorted_counts = np.sort(counts)  # Sort the counts\n",
    "        second_most_frequent_count = sorted_counts[-2]  # Get the second highest count\n",
    "        total_count = np.sum(counts)\n",
    "        if second_most_frequent_count / total_count < minFrac:\n",
    "            #print(\"The second most contributing MC true particle in this cluster contributes less than 30% hits\")\n",
    "            return eventArray\n",
    "    else:\n",
    "        print(\"there is only one MC true particle in this cluster\")\n",
    "        return eventArray\n",
    "    \n",
    "    #This double loop is filling node features and edge labels (plus some lists of input coordinates for plotting purposes)\n",
    "    for hit1 in range(0,n_hits):\n",
    "        features[hit1,0]=hx[hit1]\n",
    "        features[hit1,1]=hy[hit1]\n",
    "        features[hit1,2]=hz[hit1]\n",
    "        features[hit1,3]=hadc[hit1]\n",
    "        features[hit1,4]=hmc[hit1]\n",
    "        xPos.append(features[hit1,0])\n",
    "        yPos.append(features[hit1,1])\n",
    "        zPos.append(features[hit1,2])\n",
    "        adcCounts.append(features[hit1,3])\n",
    "\n",
    "        for hit2 in range(hit1+1,n_hits):\n",
    "            dx = hx[hit2] - hx[hit1]\n",
    "            dy = hy[hit2] - hy[hit1]\n",
    "            dz = hz[hit2] - hz[hit1]\n",
    "            dist = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "            adc_ratio = max(hadc[hit1], hadc[hit2]) / min(hadc[hit1], hadc[hit2])\n",
    "            \n",
    "            if (dist>max_hit_distance_radius):\n",
    "                continue            \n",
    "\n",
    "            edgeStart[index]=int(hit1)\n",
    "            edgeEnd[index]=int(hit2)\n",
    "            #here I am saving edge label as 1, if the two hits come from the same mc particle, and 0 otherwise:\n",
    "            if((hit1 != hit2) and (int(hmc[hit1])!=int(hmc[hit2]))):\n",
    "                edgeLabel[index]=False\n",
    "                index=index+1\n",
    "            elif((hit1 != hit2) and (int(hmc[hit1])==int(hmc[hit2]))):\n",
    "                edgeLabel[index]=True\n",
    "                index=index+1\n",
    "    \n",
    "    #If I have added a max distance radius cut (and regardless, since \"dimension\" contains double counting), I need to resize the arrays to contain entries for the hits within that range.\n",
    "    edgeLabel = edgeLabel[:index]\n",
    "    edgeStart = edgeStart[:index]\n",
    "    edgeEnd = edgeEnd[:index]\n",
    "    features = features[:index,:]\n",
    "    #print(\"N positive edges = \",np.sum(edgeLabel > 0))\n",
    "    #print(\"N negative edges = \",np.sum(edgeLabel == 0))\n",
    "    #print(\"index = \",index)    \n",
    "    \n",
    "    if edgeLabel[np.where(edgeLabel==False)].size == 0:\n",
    "        #print(\"All hits in this graph are connected!!!\")\n",
    "        return eventArray\n",
    "        \n",
    "    eventArray=[features,edgeStart,edgeEnd,edgeLabel]\n",
    "    return eventArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes the list of graphs in \"dataset\" as returned by \"process_file\", and it splits every graph in it into four subgraphs: having positive train edges, negative train edges, positive test edges, negative test edges. The split is performed based on the \"test_train_ratio\" config parameter. It happens to be the case that the graphs have more positive edges than negative edges. Negative edges are then augmented, resampling them. \n",
    "\n",
    "At the end the graphs are bundled in batches. If you wanted to do a sampling, you could use NeighbourLoader instead of data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples_for_training(dataset):\n",
    "    \n",
    "    train_data_list=[]\n",
    "    train_pos_data_list=[]\n",
    "    train_neg_data_list=[]\n",
    "    test_pos_data_list=[]\n",
    "    test_neg_data_list=[]\n",
    "    \n",
    "    for data in dataset:\n",
    "        print(\"----------------------------------------------------------------------------------------------------------\")\n",
    "        # Split edge set for training and testing\n",
    "        u = data.edge_index[0]\n",
    "        v = data.edge_index[1]\n",
    "        edge_attr = data.edge_attr\n",
    "        eids = torch.arange(data.num_edges)\n",
    "        if data.num_edges<10:\n",
    "            continue\n",
    "        \n",
    "        eids = eids[torch.randperm(eids.size(0))]\n",
    "        test_size = int(len(eids) * test_train_ratio)\n",
    "        train_size = len(eids) - test_size\n",
    "        test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "        train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]] \n",
    "        \n",
    "        # Generating negative edges for training and testing\n",
    "        adj = np.zeros((data.num_nodes, data.num_nodes))\n",
    "        adj[u, v] = data.edge_attr\n",
    "\n",
    "        neg_u=[]\n",
    "        neg_v=[]\n",
    "        i=0\n",
    "        j=0\n",
    "        for ihit in data.x:\n",
    "            j=0\n",
    "            for jhit in data.x:\n",
    "                #print(\"i = \",i,\" j = \",j)\n",
    "                if j>i:\n",
    "                    dx = jhit[0].item() - ihit[0].item()\n",
    "                    dy = jhit[1].item() - ihit[1].item()\n",
    "                    dz = jhit[2].item() - ihit[2].item()\n",
    "                    dist = np.sqrt(dx**2 + dy**2 + dz**2)\n",
    "                    if adj[i,j]!=1 and dist<max_hit_distance_radius: #need to specify that it is within radius, otherwise I am labelling as negative some edges that were actually positive but I discarded because of distance\n",
    "                        neg_u.append(i)\n",
    "                        neg_v.append(j)\n",
    "                j=j+1\n",
    "            i=i+1\n",
    "        neg_u=np.array(neg_u)    \n",
    "        neg_v=np.array(neg_v) \n",
    "        \n",
    "        print(\"data.num_edges = \",data.num_edges)\n",
    "        print(\"data.num_nodes = \",data.num_nodes)\n",
    "\n",
    "        total_neg_edges = len(neg_u)  # Total number of available negative edges\n",
    "        desired_test_size = int(data.num_edges * test_train_ratio)\n",
    "        desired_train_size = data.num_edges - desired_test_size\n",
    "\n",
    "        shuffled_neg_eids = torch.randperm(len(neg_u))\n",
    "\n",
    "        # Determine the split point for test and train sizes based on test_train_ratio\n",
    "        split_point = int(len(shuffled_neg_eids) * test_train_ratio)\n",
    "        \n",
    "        # Split the shuffled indices into training and testing segments without overlap\n",
    "        test_neg_indices = shuffled_neg_eids[:split_point]\n",
    "        train_neg_indices = shuffled_neg_eids[split_point:]\n",
    "\n",
    "        current_test_size = len(test_neg_indices)\n",
    "        current_train_size = len(train_neg_indices)\n",
    "        shortfall_test = desired_test_size - current_test_size\n",
    "        shortfall_train = desired_train_size - current_train_size\n",
    "\n",
    "        if shortfall_test > 0:\n",
    "            additional_test_indices = test_neg_indices[torch.randint(high=len(test_neg_indices), size=(shortfall_test,))]\n",
    "            test_neg_indices = torch.cat((test_neg_indices, additional_test_indices))\n",
    "\n",
    "        # Augment train_neg_indices if there's a shortfall\n",
    "        if shortfall_train > 0:\n",
    "            additional_train_indices = train_neg_indices[torch.randint(high=len(train_neg_indices), size=(shortfall_train,))]\n",
    "            train_neg_indices = torch.cat((train_neg_indices, additional_train_indices))\n",
    "\n",
    "        # Use these indices to get the corresponding negative edges for training and testing\n",
    "        test_neg_u, test_neg_v = neg_u[test_neg_indices], neg_v[test_neg_indices]\n",
    "        train_neg_u, train_neg_v = neg_u[train_neg_indices], neg_v[train_neg_indices]\n",
    "\n",
    "        if(test_neg_u.size>1):\n",
    "            test_neg_u=torch.tensor(test_neg_u)\n",
    "            test_neg_v=torch.tensor(test_neg_v)\n",
    "        else:\n",
    "            test_neg_u=torch.tensor([test_neg_u])\n",
    "            test_neg_v=torch.tensor([test_neg_v])\n",
    "        if(train_neg_u.size>1):\n",
    "            train_neg_u=torch.tensor(train_neg_u)\n",
    "            train_neg_v=torch.tensor(train_neg_v)\n",
    "        else:\n",
    "            train_neg_u=torch.tensor([train_neg_u])\n",
    "            train_neg_v=torch.tensor([train_neg_v])\n",
    "        \n",
    "        test_pos_edge_index = torch.tensor([test_pos_u.numpy(),test_pos_v.numpy()], dtype=torch.long)\n",
    "        test_neg_edge_index = torch.tensor([test_neg_u.numpy(),test_neg_v.numpy()], dtype=torch.long)\n",
    "        train_pos_edge_index = torch.tensor([train_pos_u.numpy(),train_pos_v.numpy()], dtype=torch.long)\n",
    "        train_neg_edge_index = torch.tensor([train_neg_u.numpy(),train_neg_v.numpy()], dtype=torch.long)\n",
    "\n",
    "        train_edge_index = torch.cat([train_pos_edge_index, train_neg_edge_index], dim=1)\n",
    "        test_edge_index = torch.cat([test_pos_edge_index, test_neg_edge_index], dim=1)\n",
    "\n",
    "        train_data = Data(x=data.x, edge_index=train_edge_index)\n",
    "    \n",
    "        train_pos_data_mc = Data(x=data.x, edge_index=train_pos_edge_index)\n",
    "        train_neg_data_mc = Data(x=data.x, edge_index=train_neg_edge_index)\n",
    "        test_pos_data_mc = Data(x=data.x, edge_index=test_pos_edge_index)\n",
    "        test_neg_data_mc = Data(x=data.x, edge_index=test_neg_edge_index)    \n",
    "\n",
    "        visualise_particle_projections(train_pos_data_mc)     #Showing particle projection plots\n",
    "\n",
    "        #Now I want to remove the MC truth feature that I just used in the 2D plots, as I dont want to use this in training...\n",
    "        new_features = data.x[:, :4]\n",
    "        \n",
    "        train_pos_data = Data(x=new_features, edge_index=train_pos_edge_index)\n",
    "        train_neg_data = Data(x=new_features, edge_index=train_neg_edge_index)\n",
    "        test_pos_data = Data(x=new_features, edge_index=test_pos_edge_index)\n",
    "        test_neg_data = Data(x=new_features, edge_index=test_neg_edge_index) \n",
    "\n",
    "        train_data_list.append(train_data)\n",
    "        train_pos_data_list.append(train_pos_data)\n",
    "        train_neg_data_list.append(train_neg_data)\n",
    "        test_pos_data_list.append(test_pos_data)\n",
    "        test_neg_data_list.append(test_neg_data)\n",
    "        \n",
    "\n",
    "    batchSize=128\n",
    "    train_data_loader=DataLoader(train_data_list, batch_size=batchSize, shuffle=False)    \n",
    "    train_pos_data_loader=DataLoader(train_pos_data_list, batch_size=batchSize, shuffle=False) \n",
    "    train_neg_data_loader=DataLoader(train_neg_data_list, batch_size=batchSize, shuffle=False) \n",
    "    test_pos_data_loader=DataLoader(test_pos_data_list, batch_size=batchSize, shuffle=False) \n",
    "    test_neg_data_loader=DataLoader(test_neg_data_list, batch_size=batchSize, shuffle=False) \n",
    "\n",
    "    #Redefining train_data, train_pos_data, train_neg_data, test_pos_data, test_neg_data\n",
    "    train_data=next(iter(train_data_loader))\n",
    "    train_pos_data=next(iter(train_pos_data_loader))\n",
    "    train_neg_data=next(iter(train_neg_data_loader))\n",
    "    test_pos_data=next(iter(test_pos_data_loader))\n",
    "    test_neg_data=next(iter(test_neg_data_loader))\n",
    "\n",
    "    # Print train and test sizes ########################\n",
    "    print(\"Number of positive train edges: \",train_pos_data.num_edges,\" nodes: \",train_pos_data.num_nodes)\n",
    "    print(\"Number of negative train edges: \",train_neg_data.num_edges,\" nodes: \",train_neg_data.num_nodes)\n",
    "    print(\"Number of positive test edges: \",test_pos_data.num_edges,\" nodes: \",test_pos_data.num_nodes)\n",
    "    print(\"Number of negative test edges: \",test_neg_data.num_edges,\" nodes: \",test_neg_data.num_nodes)\n",
    "    #####################################################\n",
    "    \n",
    "    return train_data, train_pos_data, train_neg_data, test_pos_data, test_neg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_by_edge_label(input_data, edge_label_value):\n",
    "    edge_attr = input_data.edge_attr\n",
    "    condition = (edge_attr == edge_label_value).squeeze()\n",
    "\n",
    "    # Create a subgraph by filtering edges based on the condition\n",
    "    subgraph_edge_index = input_data.edge_index[:, condition]\n",
    "    subgraph = Data(x=input_data.x, edge_index=subgraph_edge_index, edge_attr=edge_attr[condition])\n",
    "\n",
    "    # Get the set of nodes in the original graph and the subgraph\n",
    "    original_nodes = set(range(input_data.num_nodes))\n",
    "    subgraph_nodes = set(subgraph.edge_index.flatten().unique().numpy())\n",
    "    \n",
    "    # Find isolated nodes (nodes not connected to any edges satisfying the condition)\n",
    "    isolated_nodes = original_nodes.difference(subgraph_nodes)\n",
    "    subgraph.x = input_data.x\n",
    "\n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_info(data):\n",
    "    # Expand the tensor printing\n",
    "    torch.set_printoptions(threshold=5000)\n",
    "\n",
    "    print(\"data.num_nodes = \", data.num_nodes)\n",
    "    print(\"data.num_edges = \", data.num_edges)\n",
    "    # Number of node features\n",
    "    print(\"data.x = \",data.x)\n",
    "    # Get the edge attributes from the input graph\n",
    "    edge_attributes = data.edge_attr\n",
    "    # Count the number of edges with positive values\n",
    "    num_positive_edges = torch.sum(edge_attributes > 0).item()\n",
    "    num_negative_edges = torch.sum(edge_attributes < 1).item()\n",
    "    print(\"Number of edges with label 1:\", num_positive_edges)\n",
    "    print(\"Number of edges with label 0:\", num_negative_edges)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pos_score, neg_score):\n",
    "    \n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    \n",
    "    return loss_function(scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pos_test_score, neg_test_score):\n",
    "    scores = torch.cat([pos_test_score, neg_test_score])\n",
    "    labels = torch.cat([torch.ones(pos_test_score.shape[0]), torch.zeros(neg_test_score.shape[0])])\n",
    "    true_labels = labels.numpy()\n",
    "\n",
    "    thresholds = torch.arange(0.05, 1.0, 0.05)\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        prediction = (scores >= threshold).float()\n",
    "        accuracy.append(accuracy_score(true_labels, prediction.numpy()))\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    max_accuracy = torch.max(torch.tensor(accuracy))\n",
    "    max_accuracy_index = torch.argmax(torch.tensor(accuracy))\n",
    "    optimal_threshold = thresholds[max_accuracy_index].item()\n",
    "\n",
    "    return optimal_threshold, max_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(pos_test_score, neg_test_score, threshold):\n",
    "    scores = torch.cat([pos_test_score, neg_test_score])\n",
    "    labels = torch.cat([torch.ones(pos_test_score.shape[0]), torch.zeros(neg_test_score.shape[0])])\n",
    "    probabilities=scores\n",
    "    predictions = (probabilities >= threshold).float()\n",
    "    true_labels = labels.numpy()\n",
    "    predicted_labels =predictions.numpy()\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_labels, probabilities.numpy())\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='example estimator')\n",
    "    display.plot()\n",
    "    plt.show()\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    \n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    ax.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Positive'])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(pos_test_score, neg_test_score,train_or_test):\n",
    "    plt.hist(pos_test_score.numpy(), bins=100, range=(0, 1.1), alpha=0.3, color='blue', edgecolor='black', label='pos_test_score')\n",
    "    plt.hist(neg_test_score.numpy(), bins=100, range=(0, 1.1), alpha=0.3, color='red', edgecolor='black', label='neg_test_score')\n",
    "\n",
    "    plt.yscale('log')\n",
    "    plt.title(train_or_test)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_evolution(epochs, training_loss, test_loss, accuracy, ylim1, ylim2):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.plot(epochs, training_loss, label='Training Loss', color='b')\n",
    "    ax1.plot(epochs, test_loss, label='Validation Loss', color='g')\n",
    "\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='b')\n",
    "    ax1.tick_params('y', colors='b')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.set_title('Training and Validation Loss Over Epochs')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, accuracy, label='Accuracy', color='r')\n",
    "    ax2.set_ylabel('Accuracy Scale', color='r')\n",
    "    ax2.tick_params('y', colors='r')\n",
    "    ax2.legend(loc='upper right')\n",
    "\n",
    "    if ylim1 > 0:\n",
    "        ax1.set_ylim(0.0001, ylim1)\n",
    "    if ylim2 > 0:\n",
    "        ax2.set_ylim(0.0001, ylim2)\n",
    "    ax1.set_yscale('log')\n",
    "\n",
    "    plt.draw()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_particle_projections(data):\n",
    "    x = data.x[:,0]\n",
    "    y = data.x[:,1]\n",
    "    z = data.x[:,2]\n",
    "    adc = data.x[:,3]\n",
    "    mc = data.x[:,4]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    unique_elements, counts = np.unique(mc.numpy(), return_counts=True)\n",
    "    #print(\"mc particles in this shower = \", unique_elements, \" and their occurrences = \", counts)\n",
    "    \n",
    "    axes[0].scatter(x, y, c=mc, cmap='tab20')\n",
    "    axes[0].set_title(\"X-Y Plot\")\n",
    "    axes[0].set_xlabel(\"X\")\n",
    "    axes[0].set_ylabel(\"Y\")\n",
    "\n",
    "    # x-z plot\n",
    "    axes[1].scatter(x, z, c=mc, cmap='tab20')\n",
    "    axes[1].set_title(\"X-Z Plot\")\n",
    "    axes[1].set_xlabel(\"X\")\n",
    "    axes[1].set_ylabel(\"Z\")\n",
    "\n",
    "    # y-z plot\n",
    "    axes[2].scatter(y, z, c=mc, cmap='tab20')\n",
    "    axes[2].set_title(\"Y-Z Plot\")\n",
    "    axes[2].set_xlabel(\"Y\")\n",
    "    axes[2].set_ylabel(\"Z\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.fc = torch.nn.Linear(2*hidden_channels, 1)  # Concatenated embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.max_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_validation_accuracy = float(0)\n",
    "\n",
    "    def early_stop(self, validation_accuracy):\n",
    "        print(\"validation_accuracy = \",validation_accuracy, \"self.max_validation_accuracy = \",self.max_validation_accuracy, \"self.max_validation_accuracy - self.max_delta = \",self.max_validation_accuracy - self.max_delta)\n",
    "        if validation_accuracy > self.max_validation_accuracy:\n",
    "            self.max_validation_accuracy = validation_accuracy\n",
    "            self.counter = 0\n",
    "        elif validation_accuracy < (self.max_validation_accuracy - self.max_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter <= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience=3, min_delta=0.03)\n",
    "def train_gnn(train_data, train_pos_data, train_neg_data, test_pos_data, test_neg_data):\n",
    "    \n",
    "    training_loss = []\n",
    "    test_loss = []\n",
    "    accuracy = []\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # This is using the pyg own module ##################\n",
    "    GraphSAGE_model = GraphSAGE(\n",
    "        train_data.num_node_features,\n",
    "        hidden_channels=model_hidden_channels,\n",
    "        num_layers=model_num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    Predictor_model = Predictor(\n",
    "        hidden_channels=model_hidden_channels\n",
    "    ).to(device)\n",
    "\n",
    "    # # ----------- 3. set up loss and optimizer -------------- #\n",
    "    model_parameters = list(GraphSAGE_model.parameters()) + list(Predictor_model.parameters())\n",
    "    optimizer = torch.optim.Adam(model_parameters, lr=learning_rate)\n",
    "    \n",
    "    # # ----------- 4. training -------------------------------- #\n",
    "    all_logits = []\n",
    "    last_epoch=0\n",
    "    for e in epochs:\n",
    "         # forward\n",
    "        GraphSAGE_model.train()\n",
    "        Predictor_model.train()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Generate node embeddings\n",
    "        h = GraphSAGE_model(train_data.x, train_data.edge_index)\n",
    "    \n",
    "        # For each edge, concatenate embeddings of source and destination\n",
    "        h_pos_edges = torch.cat((h[train_pos_data.edge_index[0]], h[train_pos_data.edge_index[1]]), dim=-1)\n",
    "        h_neg_edges = torch.cat((h[train_neg_data.edge_index[0]], h[train_neg_data.edge_index[1]]), dim=-1)\n",
    "        \n",
    "        # Predictions\n",
    "        pos_pred = Predictor_model(h_pos_edges).squeeze()\n",
    "        neg_pred = Predictor_model(h_neg_edges).squeeze()\n",
    "    \n",
    "        # Calculate loss\n",
    "        loss = compute_loss(pos_pred, neg_pred)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Current test sample loss for this epoch\n",
    "        with torch.no_grad():\n",
    "            GraphSAGE_model.eval()\n",
    "            Predictor_model.eval()\n",
    "    \n",
    "            # For each edge, concatenate embeddings of source and destination\n",
    "            h_pos_test_edges = torch.cat((h[test_pos_data.edge_index[0]], h[test_pos_data.edge_index[1]]), dim=-1)\n",
    "            h_neg_test_edges = torch.cat((h[test_neg_data.edge_index[0]], h[test_neg_data.edge_index[1]]), dim=-1)\n",
    "        \n",
    "            # Predictions\n",
    "            pos_test_pred = Predictor_model(h_pos_test_edges).squeeze()\n",
    "            neg_test_pred = Predictor_model(h_neg_test_edges).squeeze()\n",
    "    \n",
    "            # Calculate loss\n",
    "            testloss = compute_loss(pos_test_pred, neg_test_pred)\n",
    "        \n",
    "        trainingloss = loss\n",
    "\n",
    "        optimal_threshold, maximum_accuracy=calculate_accuracy(pos_test_pred, neg_test_pred)           \n",
    "\n",
    "        training_loss.append(trainingloss.detach().numpy())\n",
    "        test_loss.append(testloss)\n",
    "        accuracy.append(maximum_accuracy)\n",
    "        \n",
    "        if e%5 == 0:\n",
    "            print('======> In epoch {}, training loss: {}, test loss: {}'.format(e, trainingloss, testloss))\n",
    "            print(\"optimal threshold = \",optimal_threshold,\" maximum_accuracy = \",maximum_accuracy)\n",
    "            plot_scores(pos_pred.detach(), neg_pred.detach(), \"train\")\n",
    "            plot_scores(pos_test_pred, neg_test_pred, \"test\")\n",
    "        \n",
    "        last_epoch=e\n",
    "        if e>50 and early_stopper.early_stop(maximum_accuracy.item()):     \n",
    "            break\n",
    "\n",
    "    plot_loss_evolution(range(0,last_epoch),training_loss,test_loss,accuracy,-999,-999)\n",
    "    show_confusion_matrix(pos_test_pred,neg_test_pred, optimal_threshold)\n",
    "\n",
    "    now = datetime.now()  # Current date and time\n",
    "    date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")  # Format as a string\n",
    "\n",
    "    base_filename_graphsage = 'GraphSAGE_model'\n",
    "    base_filename_predictor = 'Predictor_model'\n",
    "\n",
    "    graphsage_output_filename = f'{base_filename_graphsage}_{date_time}.pt'\n",
    "    predictor_output_filename = f'{base_filename_predictor}_{date_time}.pt'\n",
    "\n",
    "    GraphSAGE_model_jit = torch.jit.script(GraphSAGE_model.jittable())\n",
    "    Predictor_model_jit = torch.jit.script(Predictor_model)\n",
    "    GraphSAGE_model_jit.save(graphsage_output_filename)\n",
    "    Predictor_model_jit.save(predictor_output_filename)\n",
    "    \n",
    "    return GraphSAGE_model, Predictor_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the GNN! (Here are the commands to prepare samples and run the training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters (modify based on your needs!):\n",
    "**prepare_samples** : if the sample is already loaded in memory in the notebook, I don't want to accidentally re-prepare it (as it's time consuming), so I check this condition later in the notebook when I call prepare_samples_for_training\n",
    "\n",
    "**n_hits_in_graph_min, n_hits_in_graph_max** : minimum and maximum number of hits for cluster to be in input sample for training\n",
    "\n",
    "**minFrac** : fraction of hits in the cluster that are contributed by the *second* most contributing MC particle. I want there to be at least a couple of showers both contributing a significant fraction of energy. This is to avoid selecting clusters where only one particle makes almost all of the hits, and then there is only a handful of hits from other particles.\n",
    "\n",
    "**test_train_ratio**, **epochs**, **learning rate** should self-describe.\n",
    "\n",
    "***model_hidden_channels*** : the dimensionality of the hidden feature space in the GraphSAGE model\n",
    "\n",
    "***model_num_layers*** : number of message-passing layers in the GraphSAGE model\n",
    "\n",
    "***normalise_adcs***,***normalise_positions*** : bringing inputs down to a \"small number\" range. not normalised between 0 and 1, but numbers are broadly in the 1-10 range.\n",
    "\n",
    "***plot_input_distributions*** : if true, plot input node feature distributions (i.e. x,y,z,adc) plus some other \n",
    "\n",
    "***max_hit_distance_radius*** : in this edge-prediction problem, only create an edge (regardless of if positive or negative) for hits that are less than this amount apart. Units change based on normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "prepare_samples=True\n",
    "\n",
    "n_hits_in_graph_min=400\n",
    "n_hits_in_graph_max=700\n",
    "minFrac=0.3\n",
    "\n",
    "test_train_ratio=0.1\n",
    "epochs = range(1, 100)  \n",
    "learning_rate=0.01\n",
    "model_hidden_channels=16\n",
    "model_num_layers=10\n",
    "\n",
    "normalise_adcs=True\n",
    "normalise_positions=True\n",
    "\n",
    "plot_input_distributions=True\n",
    "\n",
    "if normalise_positions:\n",
    "    max_hit_distance_radius=0.1 #m \n",
    "else:\n",
    "    max_hit_distance_radius=10 #cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if prepare_samples:\n",
    "    file_path = f\"training_out_test_1000events.csv\"\n",
    "    output_directory = f\"outputDir\"\n",
    "    dataset=process_file(file_path)\n",
    "    train_data, train_pos_data, train_neg_data, test_pos_data, test_neg_data = prepare_samples_for_training(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphSAGE_model, Predictor_model=train_gnn(train_data, train_pos_data, train_neg_data, test_pos_data, test_neg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't currently used in this example notebook, but you may find it useful e.g. if you want to assess network performance separately from training, or if you add post-processing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if I need to reload the model that was already saved:\n",
    "GraphSAGE_model_new=torch.load(\"GraphSAGE_model_2024-03-07_01-02-39_10layers_10cm_89percent.pt\")\n",
    "GraphSAGE_model_new.eval()\n",
    "Predictor_model_new=torch.load(\"Predictor_model_2024-03-07_01-02-39_10layers_10cm_89percent.pt\")\n",
    "Predictor_model_new.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
